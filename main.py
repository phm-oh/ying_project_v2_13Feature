# ‡πÑ‡∏ü‡∏•‡πå: main.py
# Path: main.py
# ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: Main entry point ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î (‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß)

"""
main.py - Main entry point ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Feature Selection ‡πÅ‡∏•‡∏∞ Model Comparison Pipeline
"""

import sys
import os
import argparse
import time
from pathlib import Path

# ‡πÄ‡∏û‡∏¥‡πà‡∏° src path
sys.path.append(str(Path(__file__).parent / "src"))

# ‡πÅ‡∏Å‡πâ imports ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á circular import
import src.config as config
import src.utils as utils
from src.data_preprocessing import DataPreprocessor
from src.feature_selection import FeatureSelector
from src.model_training import ModelTrainer
from src.model_comparison import ModelComparator
from src.visualization import Visualizer

class MLPipeline:
    """Main Pipeline Class ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô ML Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.logger = utils.logger
        self.results = {}
        self.start_time = None
        
        # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        if self.verbose:
            self.print_pipeline_info()
    
    def print_pipeline_info(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á Pipeline"""
        print("=" * 80)
        print("üöÄ FEATURE SELECTION & MODEL COMPARISON PIPELINE")
        print("=" * 80)
        print(f"üìä Dataset: {config.DATA_PATH}")
        print(f"üéØ Target: {config.TARGET_COLUMN}")
        print(f"üîß Normalization: {config.NORMALIZATION_METHOD}")
        print(f"‚ú® Feature Selection: {config.FEATURE_SELECTION_METHOD}")
        print(f"ü§ñ Models: {', '.join(config.SELECTED_MODELS)}")
        print(f"üìà CV Folds: {config.CV_FOLDS}")
        print(f"üé≤ Random State: {config.RANDOM_STATE}")
        print("=" * 80)
        print()
    
    def validate_setup(self):
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÅ‡∏•‡∏∞‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"""
        self.logger.info("Validating pipeline setup...")
        
        issues = []
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        if not config.DATA_PATH.exists():
            issues.append(f"‚ùå Data file not found: {config.DATA_PATH}")
        else:
            self.logger.info(f"‚úÖ Data file found: {config.DATA_PATH}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå output
        required_dirs = [config.RESULT_DIR, config.MODELS_DIR, config.LOGS_DIR]
        for dir_path in required_dirs:
            if not dir_path.exists():
                dir_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"üìÅ Created directory: {dir_path}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö configuration
        try:
            config.validate_config()
            self.logger.info("‚úÖ Configuration validation passed")
        except ValueError as e:
            issues.append(f"‚ùå Configuration error: {str(e)}")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö dependencies
        try:
            import pandas as pd
            import numpy as np
            import sklearn
            import matplotlib.pyplot as plt
            import seaborn as sns
            self.logger.info("‚úÖ All required packages available")
        except ImportError as e:
            issues.append(f"‚ùå Missing dependency: {str(e)}")
        
        if issues:
            for issue in issues:
                print(issue)
                self.logger.error(issue)
            raise RuntimeError("Pipeline validation failed. Please fix the issues above.")
        
        self.logger.info("Pipeline validation completed successfully")
    
    def run_step_1_preprocessing(self):
        """Step 1: Data Preprocessing"""
        print("\nüîÑ STEP 1: DATA PREPROCESSING")
        print("-" * 50)
        
        try:
            preprocessor = DataPreprocessor()
            df_normalized, preprocessing_report = preprocessor.run_preprocessing()
            
            self.results['step1'] = {
                'data': df_normalized,
                'report': preprocessing_report,
                'status': 'completed'
            }
            
            print("‚úÖ Step 1 completed successfully!")
            print(f"üìä Normalized data shape: {df_normalized.shape}")
            
        except Exception as e:
            self.logger.error(f"Step 1 failed: {str(e)}")
            self.results['step1'] = {'status': 'failed', 'error': str(e)}
            raise
    
    def run_step_2_feature_selection(self):
        """Step 2: Feature Selection"""
        print("\nüéØ STEP 2: FEATURE SELECTION")
        print("-" * 50)
        
        try:
            selector = FeatureSelector()
            data_selected, selection_report = selector.run_feature_selection()
            
            self.results['step2'] = {
                'data': data_selected,
                'report': selection_report,
                'selected_features': selector.selected_features,
                'status': 'completed'
            }
            
            print("‚úÖ Step 2 completed successfully!")
            print(f"‚ú® Selected features: {len(selector.selected_features)}")
            print(f"üìà Improvement: {selection_report.get('best_method', {}).get('accuracy', 0):.4f}")
            
        except Exception as e:
            self.logger.error(f"Step 2 failed: {str(e)}")
            self.results['step2'] = {'status': 'failed', 'error': str(e)}
            raise
    
    def run_step_3_model_training(self):
        """Step 3: Model Training"""
        print("\nü§ñ STEP 3: MODEL TRAINING")
        print("-" * 50)
        
        try:
            trainer = ModelTrainer()
            models, training_report = trainer.run_model_training()
            
            self.results['step3'] = {
                'models': models,
                'report': training_report,
                'status': 'completed'
            }
            
            best_model = training_report['best_models']['best_test_accuracy']
            best_accuracy = training_report['best_models']['test_accuracy']
            
            print("‚úÖ Step 3 completed successfully!")
            print(f"üèÜ Best model: {best_model}")
            print(f"üìä Best accuracy: {best_accuracy:.4f}")
            
        except Exception as e:
            self.logger.error(f"Step 3 failed: {str(e)}")
            self.results['step3'] = {'status': 'failed', 'error': str(e)}
            raise
    
    def run_step_4_model_comparison(self):
        """Step 4: Model Comparison"""
        print("\nüìà STEP 4: MODEL COMPARISON")
        print("-" * 50)
        
        try:
            comparator = ModelComparator()
            final_report = comparator.run_comprehensive_comparison()
            
            self.results['step4'] = {
                'report': final_report,
                'status': 'completed'
            }
            
            print("‚úÖ Step 4 completed successfully!")
            print(f"üéØ Final recommendation: {final_report['recommendations']['model_selection']['recommended_model']}")
            
        except Exception as e:
            self.logger.error(f"Step 4 failed: {str(e)}")
            self.results['step4'] = {'status': 'failed', 'error': str(e)}
            raise
    
    def run_step_5_visualization(self):
        """Step 5: Visualization"""
        print("\nüìä STEP 5: VISUALIZATION")
        print("-" * 50)
        
        try:
            visualizer = Visualizer()
            plots = visualizer.create_all_visualizations()
            
            self.results['step5'] = {
                'plots': plots,
                'status': 'completed'
            }
            
            plots_created = len([p for p in plots.values() if p is not None])
            print("‚úÖ Step 5 completed successfully!")
            print(f"üìà Plots created: {plots_created}")
            
        except Exception as e:
            self.logger.error(f"Step 5 failed: {str(e)}")
            self.results['step5'] = {'status': 'failed', 'error': str(e)}
            print("‚ö†Ô∏è  Visualization failed, but pipeline can continue")
    
    def print_final_summary(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""
        total_time = time.time() - self.start_time
        
        print("\n" + "=" * 80)
        print("üéâ PIPELINE COMPLETED SUCCESSFULLY!")
        print("=" * 80)
        
        # ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
        if self.results.get('step1', {}).get('status') == 'completed':
            preprocessing_report = self.results['step1']['report']
            print(f"üìä Data Preprocessing: ‚úÖ {preprocessing_report.get('preprocessing_method', 'N/A')}")
        
        if self.results.get('step2', {}).get('status') == 'completed':
            selection_report = self.results['step2']['report']
            n_features = len(self.results['step2']['selected_features'])
            print(f"‚ú® Feature Selection: ‚úÖ {n_features} features selected")
        
        if self.results.get('step3', {}).get('status') == 'completed':
            training_report = self.results['step3']['report']
            best_model = training_report['best_models']['best_test_accuracy']
            best_accuracy = training_report['best_models']['test_accuracy']
            print(f"ü§ñ Model Training: ‚úÖ {best_model} ({best_accuracy:.4f})")
        
        if self.results.get('step4', {}).get('status') == 'completed':
            final_report = self.results['step4']['report']
            recommended_model = final_report['recommendations']['model_selection']['recommended_model']
            print(f"üìà Model Comparison: ‚úÖ {recommended_model} recommended")
        
        if self.results.get('step5', {}).get('status') == 'completed':
            plots_created = len([p for p in self.results['step5']['plots'].values() if p is not None])
            print(f"üìä Visualization: ‚úÖ {plots_created} plots created")
        
        print(f"\n‚è±Ô∏è  Total Time: {total_time:.2f} seconds")
        print(f"üìÅ Results saved to: {config.RESULT_DIR}")
        print(f"ü§ñ Models saved to: {config.MODELS_DIR}")
        print(f"üìã Logs saved to: {config.LOGS_DIR}")
        
        # ‡πÅ‡∏™‡∏î‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
        print(f"\nüìÑ Key Output Files:")
        key_files = [
            config.get_output_path('preprocessing', 'data_normalization.csv'),
            config.get_output_path('feature_selection', 'data_selection.csv'),
            config.get_output_path('evaluation', 'model_performance.csv'),
            config.get_output_path('comparison', 'final_report.json'),
            config.get_output_path('comparison', 'model_comparison.csv')
        ]
        
        for file_path in key_files:
            if file_path.exists():
                print(f"   ‚úÖ {file_path}")
            else:
                print(f"   ‚ùå {file_path}")
        
        print("=" * 80)
    
    def run_full_pipeline(self, skip_visualization: bool = False):
        """‡∏£‡∏±‡∏ô Pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        self.start_time = time.time()
        
        try:
            # Validation
            self.validate_setup()
            
            # ‡∏£‡∏±‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
            self.run_step_1_preprocessing()
            self.run_step_2_feature_selection()
            self.run_step_3_model_training()
            self.run_step_4_model_comparison()
            
            if not skip_visualization:
                self.run_step_5_visualization()
            
            # ‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
            self.print_final_summary()
            
            return self.results
            
        except Exception as e:
            self.logger.error(f"Pipeline failed: {str(e)}")
            print(f"\n‚ùå Pipeline failed at step: {str(e)}")
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô
            print("\nüìã Pipeline Status:")
            steps = ['step1', 'step2', 'step3', 'step4', 'step5']
            step_names = ['Preprocessing', 'Feature Selection', 'Model Training', 'Model Comparison', 'Visualization']
            
            for step, name in zip(steps, step_names):
                status = self.results.get(step, {}).get('status', 'not started')
                if status == 'completed':
                    print(f"   ‚úÖ {name}: Completed")
                elif status == 'failed':
                    error = self.results[step].get('error', 'Unknown error')
                    print(f"   ‚ùå {name}: Failed ({error})")
                else:
                    print(f"   ‚è∏Ô∏è  {name}: Not started")
            
            raise

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='Feature Selection ‡πÅ‡∏•‡∏∞ Model Comparison Pipeline')
    parser.add_argument('--skip-visualization', action='store_true', 
                       help='‡∏Ç‡πâ‡∏≤‡∏° visualization step')
    parser.add_argument('--quiet', action='store_true', 
                       help='‡∏•‡∏î verbose output')
    parser.add_argument('--step', type=str, choices=['1', '2', '3', '4', '5'], 
                       help='‡∏£‡∏±‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ step ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ (1=preprocessing, 2=feature_selection, 3=training, 4=comparison, 5=visualization)')
    
    args = parser.parse_args()
    
    # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ verbosity
    verbose = not args.quiet
    
    try:
        pipeline = MLPipeline(verbose=verbose)
        
        if args.step:
            # ‡∏£‡∏±‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ step ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏
            step_num = args.step
            print(f"üéØ Running only step {step_num}...")
            
            if step_num == '1':
                pipeline.run_step_1_preprocessing()
            elif step_num == '2':
                pipeline.run_step_2_feature_selection()
            elif step_num == '3':
                pipeline.run_step_3_model_training()
            elif step_num == '4':
                pipeline.run_step_4_model_comparison()
            elif step_num == '5':
                pipeline.run_step_5_visualization()
                
            print(f"‚úÖ Step {step_num} completed!")
            
        else:
            # ‡∏£‡∏±‡∏ô pipeline ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
            results = pipeline.run_full_pipeline(skip_visualization=args.skip_visualization)
            
            # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢
            if verbose and 'step4' in results and results['step4']['status'] == 'completed':
                final_report = results['step4']['report']
                recommendations = final_report['recommendations']
                
                print(f"\nüéØ FINAL RECOMMENDATIONS:")
                print(f"   üèÜ Best Model: {recommendations['model_selection']['recommended_model']}")
                print(f"   üìä Expected Accuracy: {recommendations['deployment_considerations']['accuracy_expectation']:.4f}")
                print(f"   ‚ö° Prediction Time: {recommendations['deployment_considerations']['prediction_time']:.4f}s")
                print(f"   ‚ú® Feature Selection Improvement: {recommendations['feature_selection']['effectiveness']:.2f}%")
                
        return True
        
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Pipeline interrupted by user")
        return False
    except Exception as e:
        print(f"\n‚ùå Pipeline failed with error: {str(e)}")
        utils.logger.error(f"Pipeline failed: {str(e)}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)