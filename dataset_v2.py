# -*- coding: utf-8 -*-
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

def generate_simplified_student_dataset(n_samples=1789, base_accuracy=0.80, target_after_selection=0.87, seed=42):
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ - ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ó‡∏µ‡πà academic performance
    
    ‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å‡∏Ñ‡∏ß‡∏£‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö academic performance ‡πÅ‡∏•‡∏∞ interests ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏•‡∏±‡∏Å
    ‡∏•‡∏ö lifestyle factors ‡πÅ‡∏•‡∏∞ validation features ‡∏≠‡∏≠‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô
    """
    
    np.random.seed(seed)
    departments = ['‡∏ö‡∏±‡∏ç‡∏ä‡∏µ', '‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®', '‡∏≠‡∏≤‡∏´‡∏≤‡∏£']
    
    # ========== CORE FEATURES (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏à‡∏≤‡∏Å‡πÅ‡∏ö‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö + ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏´‡∏•‡∏±‡∏Å) ==========
    # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏•‡∏±‡∏Å (1-100)
    math_score = np.round(np.random.uniform(30, 100, n_samples), 1)
    computer_score = np.round(np.random.uniform(30, 100, n_samples), 1)  
    language_score = np.round(np.random.uniform(30, 100, n_samples), 1)
    science_score = np.round(np.random.uniform(30, 100, n_samples), 1)
    art_score = np.round(np.random.uniform(30, 100, n_samples), 1)
    
    # ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏´‡∏•‡∏±‡∏Å (1-10 scale)
    logical_thinking = np.round(np.random.uniform(3, 10, n_samples), 1)
    creativity_skill = np.round(np.random.uniform(3, 10, n_samples), 1)
    problem_solving = np.round(np.random.uniform(3, 10, n_samples), 1)
    
    # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å (1-10 scale)
    interest_numbers = np.round(np.random.uniform(1, 10, n_samples), 1)
    interest_technology = np.round(np.random.uniform(1, 10, n_samples), 1)
    interest_cooking = np.round(np.random.uniform(1, 10, n_samples), 1)
    
    # ========== DEMOGRAPHIC FEATURES (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ - ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô) ==========
    np.random.seed(seed * 3 + 1000)  # ‡πÉ‡∏ä‡πâ seed ‡∏ï‡πà‡∏≤‡∏á‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
    age = np.random.randint(16, 20, n_samples)
    gender = np.random.choice([0, 1], n_samples)  # 0: ‡∏ä‡∏≤‡∏¢, 1: ‡∏´‡∏ç‡∏¥‡∏á
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame
    data = {
        # === CORE FEATURES ===
        '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå': math_score,
        '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå': computer_score,
        '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢': language_score,
        '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå': science_score,
        '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏®‡∏¥‡∏•‡∏õ‡∏∞': art_score,
        '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞': logical_thinking,
        '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå': creativity_skill,
        '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤': problem_solving,
        '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç': interest_numbers,
        '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ': interest_technology,
        '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£': interest_cooking,
        
        # === DEMOGRAPHIC FEATURES ===
        '‡∏≠‡∏≤‡∏¢‡∏∏': age,
        '‡πÄ‡∏û‡∏®': gender
    }
    
    df = pd.DataFrame(data)
    
    # ========== ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏° (‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ CORE FEATURES) ==========
    
    # 1. ‡πÅ‡∏ú‡∏ô‡∏Å‡∏ö‡∏±‡∏ç‡∏ä‡∏µ - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï + ‡∏ï‡∏£‡∏£‡∏Å‡∏∞ + ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç
    accounting_score = (
        2.0 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå'] +
        1.8 * df['‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞'] +
        1.5 * df['‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç'] +
        0.8 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢']
    )
    
    # 2. ‡πÅ‡∏ú‡∏ô‡∏Å‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏® - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå + ‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ + ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤  
    it_score = (
        2.0 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå'] +
        1.8 * df['‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ'] +
        1.5 * df['‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤'] +
        1.2 * df['‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞'] +
        0.8 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå']
    )
    
    # 3. ‡πÅ‡∏ú‡∏ô‡∏Å‡∏≠‡∏≤‡∏´‡∏≤‡∏£ - ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå + ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£ + ‡∏®‡∏¥‡∏•‡∏õ‡∏∞
    food_score = (
        2.0 * df['‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£'] +
        1.8 * df['‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå'] +
        1.5 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏®‡∏¥‡∏•‡∏õ‡∏∞'] +
        1.0 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå'] +
        0.8 * df['‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢']
    )
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° noise ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏à‡∏£‡∏¥‡∏á
    noise_strength = (1.0 - base_accuracy) * 8
    accounting_score += np.random.normal(0, noise_strength, n_samples)
    it_score += np.random.normal(0, noise_strength, n_samples)
    food_score += np.random.normal(0, noise_strength, n_samples)
    
    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÅ‡∏ú‡∏ô‡∏Å‡∏ï‡∏≤‡∏°‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
    scores = pd.DataFrame({
        '‡∏ö‡∏±‡∏ç‡∏ä‡∏µ': accounting_score,
        '‡∏™‡∏≤‡∏£‡∏™‡∏ô‡πÄ‡∏ó‡∏®': it_score,
        '‡∏≠‡∏≤‡∏´‡∏≤‡∏£': food_score
    })
    
    department = scores.idxmax(axis=1)
    df['‡πÅ‡∏ú‡∏ô‡∏Å'] = department
    
    # ========== ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û ==========
    
    # ‡πÅ‡∏ö‡πà‡∏á‡∏Å‡∏•‡∏∏‡πà‡∏° features ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö
    core_features = [
        '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏®‡∏¥‡∏•‡∏õ‡∏∞',
        '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞', '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå', '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤',
        '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£'
    ]
    
    demographic_features = [
        '‡∏≠‡∏≤‡∏¢‡∏∏', '‡πÄ‡∏û‡∏®'
    ]
    
    X = df.drop('‡πÅ‡∏ú‡∏ô‡∏Å', axis=1)
    y = df['‡πÅ‡∏ú‡∏ô‡∏Å']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    model_all = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=seed)
    model_all.fit(X_train, y_train)
    y_pred_all = model_all.predict(X_test)
    accuracy_all = accuracy_score(y_test, y_pred_all)
    
    # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö core features ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
    X_core = df[core_features]
    X_train_core, X_test_core, _, _ = train_test_split(X_core, y, test_size=0.3, random_state=seed, stratify=y)
    
    model_core = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=seed)
    model_core.fit(X_train_core, y_train)
    y_pred_core = model_core.predict(X_test_core)
    accuracy_core = accuracy_score(y_test, y_pred_core)
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    print(f"‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô {n_samples} ‡∏Ñ‡∏ô (‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢)")
    print(f"‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:")
    print(f"‚Ä¢ Core Features: {len(core_features)} features - ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô, ‡∏ó‡∏±‡∏Å‡∏©‡∏∞, ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à")  
    print(f"‚Ä¢ Demographic Features: {len(demographic_features)} features - ‡∏≠‡∏≤‡∏¢‡∏∏, ‡πÄ‡∏û‡∏®")
    print(f"‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÉ‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÅ‡∏ú‡∏ô‡∏Å:")
    print(df['‡πÅ‡∏ú‡∏ô‡∏Å'].value_counts())
    
    print(f"\nüìä ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö:")
    print(f"üîç ‡πÉ‡∏ä‡πâ Features ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ({len(X.columns)} features): {accuracy_all:.3f}")
    print(f"üéØ ‡πÉ‡∏ä‡πâ Core Features ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ({len(core_features)} features): {accuracy_core:.3f}")
    print(f"‚ú® ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á: {accuracy_core - accuracy_all:.3f}")
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á features
    feature_imp_all = None
    feature_imp_core = None
    
    if hasattr(model_all, 'feature_importances_'):
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó features
        feature_types = []
        for feature in X.columns:
            if feature in core_features:
                feature_types.append('Core')
            elif feature in demographic_features:
                feature_types.append('Demographic')  
            else:
                feature_types.append('Additional')
        
        feature_imp_all = pd.DataFrame({
            'Feature': X.columns,
            'Importance': model_all.feature_importances_,
            'Type': feature_types
        }).sort_values('Importance', ascending=False)
        
        feature_imp_core = pd.DataFrame({
            'Feature': core_features,
            'Importance': model_core.feature_importances_
        }).sort_values('Importance', ascending=False)
        
        print("\nüèÜ Top 10 Most Important Features (Random Forest Analysis):")
        print(feature_imp_all.head(10)[['Feature', 'Importance', 'Type']])
        
        print("\nüéØ Core Features Ranking:")
        print(feature_imp_core.head(11))
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Demographic Features ‡∏¢‡∏±‡∏á‡∏ï‡∏¥‡∏î Top 10 ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        top_10 = feature_imp_all.head(10)
        demographic_in_top10 = top_10[top_10['Type'] == 'Demographic']
        if len(demographic_in_top10) > 0:
            print(f"\nüìù ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏û‡∏ö Demographic Features ‡πÉ‡∏ô Top 10:")
            print(demographic_in_top10[['Feature', 'Importance']])
        else:
            print(f"\n‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: Core Features ‡∏Ñ‡∏£‡∏≠‡∏á‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏´‡∏•‡∏±‡∏Å")
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á target
        print(f"\nüîç ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡πÅ‡∏ú‡∏ô‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô:")
        dept_dist = df['‡πÅ‡∏ú‡∏ô‡∏Å'].value_counts(normalize=True) * 100
        for dept, pct in dept_dist.items():
            print(f"   {dept}: {pct:.1f}%")
    
    return df, accuracy_all, accuracy_core, feature_imp_all, feature_imp_core, core_features, demographic_features

if __name__ == "__main__":
    print("=== ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ ===")
    
    df, acc_all, acc_core, feat_imp_all, feat_imp_core, core_feat, demo_feat = generate_simplified_student_dataset()
    
    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
    df.to_csv('student_realistic_data.csv', index=False, encoding='utf-8-sig')
    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå student_realistic_data.csv ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")
    print(f"üìù Total Features: {len(df.columns)-1}")
    print(f"üìä Total Rows: {len(df)}")
    print(f"\nüéØ Dataset ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÇ‡∏ü‡∏Å‡∏±‡∏™‡∏ó‡∏µ‡πà academic performance")
    print(f"‚úÖ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Machine Learning Pipeline")