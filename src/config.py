# ‡πÑ‡∏ü‡∏•‡πå: config.py
# Path: src/config.py
# ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå: ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ (‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß - ‡πÄ‡∏û‡∏¥‡πà‡∏° Domain Knowledge Rules)

"""
config.py - ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß)
"""

import os
from pathlib import Path
from typing import Dict, List

# ==================== PATH SETTINGS ====================
# Base directories
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
SRC_DIR = BASE_DIR / "src"
RESULT_DIR = BASE_DIR / "result"
MODELS_DIR = BASE_DIR / "models"
LOGS_DIR = BASE_DIR / "logs"

# Data paths
RAW_DATA_DIR = DATA_DIR / "raw"
PROCESSED_DATA_DIR = DATA_DIR / "processed"
# ‡πÅ‡∏Å‡πâ path ‡πÉ‡∏´‡πâ‡∏ä‡∏µ‡πâ‡πÑ‡∏õ‡∏ó‡∏µ‡πà root directory
DATA_PATH = BASE_DIR / "student_realistic_data.csv"

# Result paths
PREPROCESSING_RESULT_DIR = RESULT_DIR / "preprocessing"
FEATURE_SELECTION_RESULT_DIR = RESULT_DIR / "feature_selection"
MODELS_RESULT_DIR = RESULT_DIR / "models"
EVALUATION_RESULT_DIR = RESULT_DIR / "evaluation"
COMPARISON_RESULT_DIR = RESULT_DIR / "comparison"

# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
for directory in [DATA_DIR, RESULT_DIR, MODELS_DIR, LOGS_DIR, 
                  RAW_DATA_DIR, PROCESSED_DATA_DIR,
                  PREPROCESSING_RESULT_DIR, FEATURE_SELECTION_RESULT_DIR,
                  MODELS_RESULT_DIR, EVALUATION_RESULT_DIR, COMPARISON_RESULT_DIR]:
    directory.mkdir(parents=True, exist_ok=True)

# ==================== DATA SETTINGS ====================
TARGET_COLUMN = "‡πÅ‡∏ú‡∏ô‡∏Å"
RANDOM_STATE = 42
TEST_SIZE = 0.2
VALIDATION_SIZE = 0.2

# Feature groups (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå)
CORE_FEATURES = [
    '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢', 
    '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏®‡∏¥‡∏•‡∏õ‡∏∞', '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞',
    '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå', '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤',
    '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£'
]

DEMOGRAPHIC_FEATURES = [
    '‡∏≠‡∏≤‡∏¢‡∏∏', '‡πÄ‡∏û‡∏®', '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', '‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á'
]

LIFESTYLE_FEATURES = [
    '‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≠‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢', '‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•‡∏°‡∏µ‡πÄ‡∏î‡∏µ‡∏¢',
    '‡∏ä‡∏≠‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠', '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏û‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏≠‡∏ö'
]

VALIDATION_FEATURES = [
    '‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ', 'GPA', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å', '‡∏´‡∏≤‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏´‡∏°'
]

# Features ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á exclude ‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ ML (‡πÄ‡∏õ‡πá‡∏ô validation data)
EXCLUDE_FEATURES = ['‡∏ä‡∏±‡πâ‡∏ô‡∏õ‡∏µ', 'GPA', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏∂‡∏á‡∏û‡∏≠‡πÉ‡∏à‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏Å', '‡∏´‡∏≤‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÉ‡∏´‡∏°‡πà‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å‡πÄ‡∏î‡∏¥‡∏°‡πÑ‡∏´‡∏°']

# ==================== DOMAIN KNOWLEDGE CONSTRAINTS (‡πÉ‡∏´‡∏°‡πà) ====================
# Feature priorities ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô ‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤

FEATURE_PRIORITIES = {
    'CORE_ACADEMIC': {
        'features': [
            '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢', 
            '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå', '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏®‡∏¥‡∏•‡∏õ‡∏∞'
        ],
        'weight': 1.0,
        'min_percentage': 40,  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 40% ‡∏Ç‡∏≠‡∏á selected features
        'description': '‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏•‡∏±‡∏Å - ‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å',
        'educational_theory': 'Academic Achievement Theory - ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à'
    },
    'CORE_SKILLS': {
        'features': [
            '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞', '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå', '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤'
        ],
        'weight': 0.9,
        'min_percentage': 15,  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 15%
        'description': '‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏´‡∏•‡∏±‡∏Å - ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ',
        'educational_theory': 'Cognitive Skills Framework - ‡∏ó‡∏±‡∏Å‡∏©‡∏∞‡∏Å‡∏≤‡∏£‡∏Ñ‡∏¥‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç'
    },
    'CORE_INTERESTS': {
        'features': [
            '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡πÄ‡∏ó‡∏Ñ‡πÇ‡∏ô‡πÇ‡∏•‡∏¢‡∏µ', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏≠‡∏≤‡∏´‡∏≤‡∏£'
        ],
        'weight': 0.8,
        'min_percentage': 15,  # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 15%
        'description': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å - ‡πÅ‡∏£‡∏á‡∏à‡∏π‡∏á‡πÉ‡∏à‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô',
        'educational_theory': 'Vocational Interest Theory - ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ'
    },
    'DEMOGRAPHIC_LIMITED': {
        'features': [
            '‡∏≠‡∏≤‡∏¢‡∏∏', '‡πÄ‡∏û‡∏®'  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏à‡∏£‡∏¥‡∏á‡πÜ
        ],
        'weight': 0.3,
        'max_percentage': 15,  # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 15%
        'description': '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏Å‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå - ‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏ô‡πâ‡∏≠‡∏¢',
        'educational_theory': 'Demographic factors should be minimized for equity'
    },
    'SOCIOECONOMIC_BLOCKED': {
        'features': [
            '‡∏£‡∏≤‡∏¢‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß', '‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏û‡∏µ‡πà‡∏ô‡πâ‡∏≠‡∏á'
        ],
        'weight': 0.0,
        'max_percentage': 0,  # ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
        'description': '‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏®‡∏£‡∏©‡∏ê‡∏Å‡∏¥‡∏à‡∏™‡∏±‡∏á‡∏Ñ‡∏° - ‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤',
        'educational_theory': 'Educational Equity Principle - ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥'
    },
    'LIFESTYLE_BLOCKED': {
        'features': [
            '‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏Å‡∏≤‡∏£‡∏ô‡∏≠‡∏ô', '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢', '‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏ã‡πÄ‡∏ä‡∏µ‡∏¢‡∏•‡∏°‡∏µ‡πÄ‡∏î‡∏µ‡∏¢',
            '‡∏ä‡∏≠‡∏ö‡∏≠‡πà‡∏≤‡∏ô‡∏´‡∏ô‡∏±‡∏á‡∏™‡∏∑‡∏≠', '‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡πÄ‡∏û‡∏•‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏≠‡∏ö'
        ],
        'weight': 0.0,
        'max_percentage': 0,  # ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡πÄ‡∏î‡πá‡∏î‡∏Ç‡∏≤‡∏î
        'description': '‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÑ‡∏•‡∏ü‡πå‡∏™‡πÑ‡∏ï‡∏•‡πå - ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ú‡∏ô‡∏Å',
        'educational_theory': 'Personal lifestyle should not determine educational opportunities'
    }
}

# Feature Selection Validation Rules (‡πÉ‡∏´‡∏°‡πà)
FEATURE_SELECTION_VALIDATION = {
    'min_core_percentage': 70,         # ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 70% ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô core features
    'max_demographic_percentage': 15,  # ‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 15% demographic
    'max_socioeconomic_percentage': 0, # ‡∏´‡πâ‡∏≤‡∏° socioeconomic features
    'max_lifestyle_percentage': 0,     # ‡∏´‡πâ‡∏≤‡∏° lifestyle features
    'min_quality_score': 150,          # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ (‡∏à‡∏≤‡∏Å 200)
    'require_all_subject_types': True, # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ß‡∏¥‡∏ä‡∏≤‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
    'require_interest_diversity': True, # ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ô‡πÉ‡∏à‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
    'academic_defensibility_threshold': 150  # ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ
}

# ==================== PREPROCESSING SETTINGS ====================
NORMALIZATION_METHODS = {
    'standard': 'StandardScaler',      # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏Å‡∏ï‡∏¥
    'minmax': 'MinMaxScaler',         # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers ‡∏ô‡πâ‡∏≠‡∏¢  
    'robust': 'RobustScaler',         # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ outliers ‡πÄ‡∏¢‡∏≠‡∏∞
    'quantile': 'QuantileTransformer' # ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏õ‡∏Å‡∏ï‡∏¥
}

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ normalization (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
NORMALIZATION_METHOD = 'standard'

# ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ missing values
MISSING_VALUE_STRATEGY = 'mean'  # 'mean', 'median', 'mode', 'drop'

# ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ categorical variables
CATEGORICAL_ENCODING = 'label'   # 'label', 'onehot', 'target'

# ==================== FEATURE SELECTION SETTINGS (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß) ====================
FEATURE_SELECTION_METHODS = {
    'enhanced_forward': 'Enhanced Sequential Forward with Domain Knowledge',
    'forward': 'SequentialFeatureSelector with forward direction',
    'backward': 'SequentialFeatureSelector with backward direction', 
    'rfe': 'Recursive Feature Elimination',
    'rfe_cv': 'RFE with Cross Validation',
    'univariate': 'Univariate Statistical Tests',
    'lasso': 'LASSO Regularization',
    'rf_importance': 'Random Forest Feature Importance'
}

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ feature selection (‡πÉ‡∏ä‡πâ enhanced version)
FEATURE_SELECTION_METHOD = 'enhanced_forward'  # ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô enhanced version

# ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (‡∏õ‡∏£‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î‡∏Ç‡∏∂‡πâ‡∏ô)
N_FEATURES_TO_SELECT = 12  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 15 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ü‡πâ‡∏ô‡πÄ‡∏≠‡∏≤‡∏î‡∏µ‡πÜ
MIN_FEATURES = 8
MAX_FEATURES = 15

# Scoring metric ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature selection
FEATURE_SELECTION_SCORING = 'accuracy'

# ==================== MODEL SETTINGS (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß) ====================
AVAILABLE_MODELS = {
    'random_forest': {
        'name': 'Random Forest',
        'class': 'RandomForestClassifier',
        'params': {
            'n_estimators': 200,       # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 100
            'max_depth': 8,            # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 10 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô overfitting
            'min_samples_split': 10,   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 5
            'min_samples_leaf': 5,     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 2
            'max_features': 'sqrt',    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î noise
            'random_state': RANDOM_STATE,
            'n_jobs': -1
        }
    },
    'gradient_boosting': {
        'name': 'Gradient Boosting',
        'class': 'GradientBoostingClassifier', 
        'params': {
            'n_estimators': 150,       # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 100
            'learning_rate': 0.08,     # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 0.1
            'max_depth': 5,            # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 6
            'min_samples_split': 15,   # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 5
            'min_samples_leaf': 8,     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 2
            'subsample': 0.8,          # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î overfitting
            'random_state': RANDOM_STATE
        }
    },
    'logistic_regression': {
        'name': 'Logistic Regression',
        'class': 'LogisticRegression',
        'params': {
            'max_iter': 2000,          # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 1000
            'C': 0.5,                  # ‡πÄ‡∏û‡∏¥‡πà‡∏° regularization (‡∏•‡∏î‡∏à‡∏≤‡∏Å 1.0)
            'random_state': RANDOM_STATE,
            'n_jobs': -1,
            'solver': 'lbfgs',
            'class_weight': 'balanced' # ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ class imbalance
        }
    },
    'svm': {
        'name': 'Support Vector Machine',
        'class': 'SVC',
        'params': {
            'kernel': 'rbf',
            'C': 0.8,                  # ‡∏•‡∏î‡∏à‡∏≤‡∏Å 1.0
            'gamma': 'scale',          # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏° complexity
            'random_state': RANDOM_STATE,
            'probability': True
        }
    },
    'xgboost': {
        'name': 'XGBoost',
        'class': 'XGBClassifier',
        'params': {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6,
            'random_state': RANDOM_STATE,
            'n_jobs': -1
        }
    }
}

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡πÄ‡∏ô‡πâ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£)
SELECTED_MODELS = ['random_forest', 'gradient_boosting', 'logistic_regression']

# ==================== CROSS VALIDATION SETTINGS (‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÅ‡∏•‡πâ‡∏ß) ====================
CV_FOLDS = 15  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏à‡∏≤‡∏Å 10 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
CV_SCORING_METRICS = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']
CV_N_JOBS = -1

# ==================== EVALUATION SETTINGS ====================
EVALUATION_METRICS = [
    'accuracy', 'precision', 'recall', 'f1_score', 
    'confusion_matrix', 'classification_report'
]

# ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ statistical testing
STATISTICAL_TESTS = {
    'paired_ttest': True,      # Paired t-test
    'wilcoxon': True,          # Wilcoxon signed-rank test
    'friedman': True,          # Friedman test (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•)
}

SIGNIFICANCE_LEVEL = 0.05

# ==================== VISUALIZATION SETTINGS ====================
PLOT_SETTINGS = {
    'figsize': (12, 8),
    'dpi': 300,
    'style': 'seaborn-v0_8',
    'palette': 'Set2',
    'font_size': 12,
    'title_size': 14,
    'save_format': 'png'
}

GENERATE_PLOTS = True
PLOT_TYPES = [
    'confusion_matrix',
    'feature_importance', 
    'performance_comparison',
    'cross_validation_scores',
    'learning_curves',
    'domain_knowledge_validation'  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
]

# ==================== OUTPUT SETTINGS ====================
SAVE_MODELS = True
SAVE_PREDICTIONS = True
SAVE_PROBABILITIES = True

# ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
OUTPUT_FORMATS = {
    'data': 'csv',           # csv, parquet, pickle
    'models': 'pickle',      # pickle, joblib
    'reports': 'json',       # json, yaml
    'plots': 'png'          # png, pdf, svg
}

# ==================== LOGGING SETTINGS ====================
LOGGING_CONFIG = {
    'level': 'INFO',         # DEBUG, INFO, WARNING, ERROR
    'format': '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    'file': LOGS_DIR / 'pipeline.log',
    'console': True
}

VERBOSE = True

# ==================== PERFORMANCE SETTINGS ====================
N_JOBS = -1                  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô CPU cores ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ (-1 = ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
MEMORY_LIMIT = '4GB'         # ‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ memory
CACHE_SIZE = 200             # Cache size ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö SVM

# ==================== ADVANCED SETTINGS ====================
# Grid Search settings (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö hyperparameter tuning)
GRID_SEARCH_PARAMS = {
    'random_forest': {
        'n_estimators': [100, 150, 200],
        'max_depth': [6, 8, 10, None],
        'min_samples_split': [5, 10, 15]
    },
    'gradient_boosting': {
        'n_estimators': [100, 150, 200],
        'learning_rate': [0.05, 0.08, 0.1],
        'max_depth': [4, 5, 6]
    },
    'logistic_regression': {
        'C': [0.1, 0.5, 1.0, 2.0],
        'solver': ['lbfgs', 'liblinear']
    }
}

ENABLE_GRID_SEARCH = False   # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î grid search
GRID_SEARCH_CV = 5
GRID_SEARCH_SCORING = 'accuracy'

# Early stopping settings
EARLY_STOPPING = {
    'enabled': False,
    'patience': 10,
    'min_delta': 0.001
}

# ==================== EXPERIMENT TRACKING ====================
EXPERIMENT_NAME = "enhanced_feature_selection_department_recommendation"
EXPERIMENT_VERSION = "v2.0"  # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó version
TRACK_EXPERIMENTS = False    # ‡πÄ‡∏õ‡∏¥‡∏î/‡∏õ‡∏¥‡∏î experiment tracking (MLflow, Weights & Biases)

# ==================== DOMAIN KNOWLEDGE FUNCTIONS (‡πÉ‡∏´‡∏°‡πà) ====================

def validate_feature_priorities(selected_features: List[str]) -> Dict:
    """
    ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ feature selection ‡∏ï‡∏£‡∏á‡∏ï‡∏≤‡∏° domain knowledge ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
    """
    validation_results = {}
    total_features = len(selected_features)
    
    if total_features == 0:
        return {'overall_valid': False, 'error': 'No features selected'}
    
    for priority_name, config in FEATURE_PRIORITIES.items():
        matching_features = [f for f in selected_features if f in config['features']]
        percentage = len(matching_features) / total_features * 100
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö constraints
        if 'min_percentage' in config:
            meets_min = percentage >= config['min_percentage']
        else:
            meets_min = True
            
        if 'max_percentage' in config:
            meets_max = percentage <= config['max_percentage']
        else:
            meets_max = True
        
        validation_results[priority_name] = {
            'matching_features': matching_features,
            'count': len(matching_features),
            'percentage': percentage,
            'meets_min_requirement': meets_min,
            'meets_max_requirement': meets_max,
            'is_valid': meets_min and meets_max,
            'config': config
        }
    
    # Overall validation
    overall_valid = all(result['is_valid'] for result in validation_results.values())
    
    return {
        'overall_valid': overall_valid,
        'priority_results': validation_results,
        'selected_features': selected_features,
        'total_features': total_features
    }

def get_feature_priority_score(selected_features: List[str]) -> float:
    """
    ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á feature selection ‡∏ï‡∏≤‡∏° domain knowledge (0-200)
    """
    if not selected_features:
        return 0.0
    
    total_score = 0
    max_possible_score = 0
    
    for priority_name, config in FEATURE_PRIORITIES.items():
        matching_features = [f for f in selected_features if f in config['features']]
        
        if config['weight'] > 0:  # ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ categories ‡∏ó‡∏µ‡πà‡∏°‡∏µ weight ‡∏ö‡∏ß‡∏Å
            feature_ratio = len(matching_features) / max(len(config['features']), 1)
            
            # ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö priority ‡∏ô‡∏µ‡πâ
            priority_score = feature_ratio * config['weight'] * 100
            total_score += priority_score
            max_possible_score += config['weight'] * 100
        else:  # Categories ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á block (weight = 0)
            if len(matching_features) > 0:
                # ‡∏´‡∏±‡∏Å‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ñ‡πâ‡∏≤‡∏°‡∏µ blocked features
                total_score -= len(matching_features) * 20  # ‡∏´‡∏±‡∏Å 20 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≠ blocked feature
    
    # Normalize to 0-200 scale
    if max_possible_score > 0:
        base_score = (total_score / max_possible_score) * 200
    else:
        base_score = 0
    
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÇ‡∏ö‡∏ô‡∏±‡∏™‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ blocked features
    blocked_categories = ['SOCIOECONOMIC_BLOCKED', 'LIFESTYLE_BLOCKED']
    no_blocked_bonus = 0
    for cat in blocked_categories:
        matching = [f for f in selected_features if f in FEATURE_PRIORITIES[cat]['features']]
        if len(matching) == 0:
            no_blocked_bonus += 25  # ‡πÇ‡∏ö‡∏ô‡∏±‡∏™ 25 ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏ï‡πà‡∏≠ category ‡∏ó‡∏µ‡πà clean
    
    final_score = base_score + no_blocked_bonus
    return max(0, min(200, final_score))  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô 0-200

def get_academic_justification(selected_features: List[str]) -> str:
    """
    ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö feature selection
    """
    validation = validate_feature_priorities(selected_features)
    quality_score = get_feature_priority_score(selected_features)
    
    # ‡∏™‡∏£‡∏∏‡∏õ‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô
    core_pct = (
        validation['priority_results']['CORE_ACADEMIC']['percentage'] +
        validation['priority_results']['CORE_SKILLS']['percentage'] +
        validation['priority_results']['CORE_INTERESTS']['percentage']
    )
    
    blocked_count = (
        len(validation['priority_results']['SOCIOECONOMIC_BLOCKED']['matching_features']) +
        len(validation['priority_results']['LIFESTYLE_BLOCKED']['matching_features'])
    )
    
    justification = f"""
üéì **Academic Justification for Feature Selection**

üìä **Selection Quality:**
- Overall Quality Score: {quality_score:.1f}/200
- Educational Factors: {core_pct:.1f}%
- Blocked Inappropriate Factors: {blocked_count} features eliminated

üìö **Educational Principles Applied:**
1. **Academic Achievement Theory**: Academic scores predict departmental success
2. **Cognitive Skills Framework**: Thinking skills are foundational for learning
3. **Vocational Interest Theory**: Career interests drive motivation and persistence
4. **Educational Equity Principle**: Avoid socioeconomic and lifestyle discrimination

‚úÖ **Defensibility:**
- Features selected based on educationally relevant factors
- Personal lifestyle factors appropriately excluded
- Socioeconomic barriers minimized for educational equity
- Selection process follows established educational psychology principles

üéØ **Practical Implementation:**
- Students can develop academic performance through effort
- Skills and interests are educationally relevant and fair
- Selection supports equal educational opportunities
- Methodology is transparent and academically sound
    """
    
    return justification.strip()

# ==================== VALIDATION CHECKS ====================
def validate_config():
    """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á configuration"""
    errors = []
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    if not DATA_PATH.exists():
        errors.append(f"Data file not found: {DATA_PATH}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö model selection
    for model in SELECTED_MODELS:
        if model not in AVAILABLE_MODELS:
            errors.append(f"Unknown model: {model}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö feature selection
    if FEATURE_SELECTION_METHOD not in FEATURE_SELECTION_METHODS:
        errors.append(f"Unknown feature selection method: {FEATURE_SELECTION_METHOD}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô features
    if N_FEATURES_TO_SELECT < MIN_FEATURES or N_FEATURES_TO_SELECT > MAX_FEATURES:
        errors.append(f"N_FEATURES_TO_SELECT must be between {MIN_FEATURES} and {MAX_FEATURES}")
    
    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö domain knowledge settings
    if not FEATURE_PRIORITIES:
        errors.append("FEATURE_PRIORITIES not defined")
    
    if not FEATURE_SELECTION_VALIDATION:
        errors.append("FEATURE_SELECTION_VALIDATION not defined")
    
    if errors:
        raise ValueError("Configuration errors:\n" + "\n".join(errors))
    
    return True

# ==================== HELPER FUNCTIONS ====================
def get_model_config(model_name):
    """‡∏î‡∏∂‡∏á configuration ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    if model_name not in AVAILABLE_MODELS:
        raise ValueError(f"Unknown model: {model_name}")
    return AVAILABLE_MODELS[model_name]

def get_output_path(category, filename):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå output"""
    if category == 'preprocessing':
        return PREPROCESSING_RESULT_DIR / filename
    elif category == 'feature_selection':
        return FEATURE_SELECTION_RESULT_DIR / filename
    elif category == 'models':
        return MODELS_RESULT_DIR / filename
    elif category == 'evaluation':
        return EVALUATION_RESULT_DIR / filename
    elif category == 'comparison':
        return COMPARISON_RESULT_DIR / filename
    else:
        return RESULT_DIR / filename

def get_model_save_path(model_name):
    """‡∏™‡∏£‡πâ‡∏≤‡∏á path ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•"""
    return MODELS_DIR / f"{model_name}.{OUTPUT_FORMATS['models']}"

# ==================== EXPORT SETTINGS ====================
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ import ‡πÉ‡∏ô modules ‡∏≠‡∏∑‡πà‡∏ô
__all__ = [
    'DATA_PATH', 'TARGET_COLUMN', 'RANDOM_STATE', 'TEST_SIZE', 'VALIDATION_SIZE',
    'CORE_FEATURES', 'DEMOGRAPHIC_FEATURES', 'LIFESTYLE_FEATURES', 'VALIDATION_FEATURES',
    'EXCLUDE_FEATURES',
    'FEATURE_PRIORITIES', 'FEATURE_SELECTION_VALIDATION',  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
    'NORMALIZATION_METHOD', 'MISSING_VALUE_STRATEGY', 'CATEGORICAL_ENCODING',
    'FEATURE_SELECTION_METHOD', 'N_FEATURES_TO_SELECT', 'MIN_FEATURES', 'MAX_FEATURES',
    'FEATURE_SELECTION_SCORING',
    'SELECTED_MODELS', 'AVAILABLE_MODELS',
    'CV_FOLDS', 'CV_SCORING_METRICS', 'CV_N_JOBS',
    'EVALUATION_METRICS', 'STATISTICAL_TESTS', 'SIGNIFICANCE_LEVEL',
    'PLOT_SETTINGS', 'GENERATE_PLOTS', 'PLOT_TYPES',
    'SAVE_MODELS', 'SAVE_PREDICTIONS', 'SAVE_PROBABILITIES',
    'OUTPUT_FORMATS', 'LOGGING_CONFIG', 'VERBOSE',
    'N_JOBS', 'MEMORY_LIMIT', 'CACHE_SIZE',
    'GRID_SEARCH_PARAMS', 'ENABLE_GRID_SEARCH', 'GRID_SEARCH_CV', 'GRID_SEARCH_SCORING',
    'EARLY_STOPPING', 'EXPERIMENT_NAME', 'EXPERIMENT_VERSION', 'TRACK_EXPERIMENTS',
    'get_model_config', 'get_output_path', 'get_model_save_path',
    'validate_config', 'COMPARISON_RESULT_DIR',
    'validate_feature_priorities', 'get_feature_priority_score', 'get_academic_justification'  # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÉ‡∏´‡∏°‡πà
]

# ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÄ‡∏°‡∏∑‡πà‡∏≠ import module
if __name__ != "__main__":
    try:
        validate_config()
    except ValueError as e:
        print(f"‚ö†Ô∏è  Configuration validation failed: {e}")
        print("Please fix the configuration before running the pipeline.")